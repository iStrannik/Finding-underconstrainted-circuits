\documentclass[a4paper]{article}
\usepackage[14pt]{extsizes}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{setspace,amsmath}
\usepackage[left=20mm, top=15mm, right=15mm, bottom=15mm, nohead, footskip=10mm]{geometry}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}
\usepackage{graphicx}
\graphicspath{{./}}
\graphicspath{{./pictures/}}
\DeclareGraphicsExtensions{.pdf,.png,.jpg}
\usepackage[tableposition=top,singlelinecheck=false]{caption}
\usepackage{subcaption}
\DeclareCaptionLabelFormat{gostfigure}{Рисунок #2}
\captionsetup*[figure]{labelformat=gostfigure, justification=centering}
\usepackage{amsfonts}

\begin{document}
 
\begin{center}
\includegraphics{MSU}

\hfill \break
\normalsize{Московский государственный университет имени М.В. Ломоносова}\\
\normalsize{Факультет вычислительной математики и кибернетики}\\
\normalsize{Кафедра информационной безопаснсти}\\
\normalsize{Лаборатория безопасности информационных систем}\\
 \hfill \break
\normalsize{Николайчук Артём Константинович}\\
\hfill\break
\hfill \break
\hfill \break
\hfill \break
\large{Обзор методов тестирования zk circuits для проверки полноты их ограничений}\\
\hfill \break
\hfill \break
\hfill \break
\normalsize{Курсовая работа}\\
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\hfill \break
\begin{flushright}
    \normalsize{Научный руководитель:}\\
    \normalsize{м.н.с}\\
    \normalsize{М.С.Воронов}\\
\end{flushright}
\end{center}
\vspace*{\fill}
\begin{center} Москва, 2024 \end{center}
\thispagestyle{empty}
 
\newpage
\section*{Аннотация}
\indent

В настоящее время всё большую популярность получают решения второго уровня(L2) на блокчейне. L2 уровень на блокчейне обозначает второй уровень масштабируемости и решает проблемы масштабирования и скорости транзакций, которые могут возникать на первом уровне блокчейна (L1).
L2 может быть представлен различными протоколами, такими как Lightning Network для биткоина или Polygon для Ethereum. Эти протоколы позволяют совершать децентрализованные транзакции и выполнять смарт-контракты вне цепи блоков, что уменьшает нагрузку на основную сеть и повышает ее масштабируемость. L2 уровень также обеспечивает более быстрые и дешевые транзакции, что делает использование блокчейна более удобным и доступным для всех участников сети. В таких протоколах часто используют zk circuit - схемы, которые позволяют доказывать, что какие-то действия были выполнены пользователем, например что пользователь совершил транзакцию токена со своего аккаунта, либо то, что пользователь знает какой-то определённый секрет. Цена ошибки или бага в таких схемах может быть невероятно высокой - потенциальные уязвимости могут быть использованы для кражи всех токенов протокола. Допустить ошибку при разработке легко - иногда достаточно забыть добавить одно условие. Для поиска недостаточно ограниченных схем разумно применять методы автоматического тестирования. Такие методы рассматриваются в этой работе.

\newpage
    \tableofcontents
\newpage
 
\newpage
\section{Введение}

\subsection{zk-rollups}
\indent

Fuzz-тестирование - способ автоматического тестирования программного обеспечения. Фаззер генерирует случайные входные данные и улучшает, или изменяет их, затем анализирует работу программы на этих данных, и пытается обнаружить потенциальные дефекты или уязвимости программного обеспечения. Фаззеры принято классифицировать по принципу генерации данных\footnote{\href{https://habr.com/ru/company/dsec/blog/517596/\#chto-takoe-fazzing}{https://habr.com/ru/company/dsec/blog/517596/\#chto-takoe-fazzing}}:

\begin{itemize}
\item Мутационные фаззеры обрабатывают заранее подготовленное множество входных данных. Наиболее популярными изменениями являются заимствованные из биологии мутации и скрещивания. Мутации - это изменение какой-то части входных данных на случайную. При скрещивании выбираются два примера, которые обмениваются друг с другом частью данных.
\item Генерационные фаззеры создают новые примеры, основываясь на информации о требуемой структуре входных данных. 
\item Смешанные фаззеры объединяют в себе два предыдущих подхода. Например, при мутации данные могут меняться не на случайные, а на сгенерированные. Или фаззер может сначала создать пул тестовых данных и к нему применять мутационный метод. 
\end{itemize}

\subsection{Представление входных данных}
\indent

На практике оказалось очень удобно задавать структуру входных данных с помощью грамматик (Пример грамматики на рис. \ref{SimpleGrammar}). Если известна грамматика, то все возможные входные данные можно представить абстрактным синтаксическим деревом (далее АST - Abstract Syntax Tree). Пример AST представлен на рисунке \ref{SimpleAST}. Это позволяет избегать синтаксических ошибок на этапе запуска программы. В дальнейшем будет показано, что такое представление полезно при генерации и мутации данных.

\begin{figure}[ht!]
\includegraphics[width=180mm]{Expressions_Grammar.png}
\caption{Грамматика арифметических выражений}
\label{SimpleGrammar}
\end{figure}

\begin{figure}[ht!]
\includegraphics[width=100mm]{SimpleAST.png}
\caption{AST для выражения 1 + (2 * 3)}
\label{SimpleAST}
\end{figure}

\newpage
\section{Цель работы}
\indent

Цель данной работы - сделать обзор значимых методов и средств, используемых при fuzz-тестировании программ, обрабатывающих структурированные входные данные. Для достижения этой цели требуется:

\begin{itemize}
\item Сформировать критерии для сравнения методов.
\item Проанализировать существующие решения по выделенным критериям.
\item Дать оценку каждому критерию.
\end{itemize}

\newpage
\section{Анализ предметной области}
\indent
 
Базовый алогоритм\footnote[1]{\href{https://www.fuzzingbook.org/html/Fuzzer.html}{https://www.fuzzingbook.org/html/Fuzzer.html}} работы любого фаззера состоит из шагов:

\begin{enumerate}
    \item Предобработка тестируемого программного обеспечения(далле SUT - Soft Under Test). Например, она может заключаться в компиляции со специальными флагами или подготовке окружения для тестирования. 
    \item Получить начальное множество входных данных и создать из него пул тестов.
    \item Выбрать из пула один или несколько примеров и получить результат их тестирования в SUT.
    \item Решить будет ли полезен этот тест в будущем.
    \item Мутировать тест и добавить его в пул.
    \item Перейти к шагу 3.
\end{enumerate}

\subsection{Получение начального множества тестов}
\indent

Первый способ получения входных данных - использовать тесты, которые написали разработчики для SUT. Этот способ позволяет сразу получить хорошее покрытие кода. При обнаружении бага разработчики исправляют его и часто добавляют тест, который проверяет работоспособность программы в этом месте. В этом смысле тесты, как начальное множество, позволяют сразу добираться до "слабых" мест в SUT. Минус этого способа - тесты разработчиков не всегда доступны.
\indent

Второй вариант - собрать пул тестов из примеров в интернете. Например, в случае fuzz-тестировании интерпретатора javascript можно в начальное множество добавлять примеры javasrcipt кода с гитхаба. Таким способом можно получить широкий пул тестов.
\indent

Третий - можно сгенерировать множество самостоятельно. В этом случае появляется возможность подтолкнуть фаззер в определённом направлении. Детали процесса генерации описаны ниже.

\subsection{Генерация тестов}
\indent

\subsubsection{Способы генерации}
\indent

Алгоритм создания тестовых данных опирается на знание их структуры. Как было отмечено ранее, эту структуру удобно задавать контекстно-свободной грамматикой. В этом случае процесс генерации нового теста заключается в построении его AST. Шаги алгоритма\footnote[1]{\href{https://www.fuzzingbook.org/html/Grammars.html\#A-Simple-Grammar-Fuzzer}{https://www.fuzzingbook.org/html/Grammars.html\#A-Simple-Grammar-Fuzzer}}:

\begin{enumerate}
    \item Положить в корень дерева стартовую вершину и добавить её в очередь вершин.
    \item Взять текущую вершину из очереди.
    \item Если текущая вершина - терминальная, то перейти к следующей вершине.
    \item Каким-то способом выбрать продукцию из правила вывода для текущего нетерминала и добавить все символы из неё в очередь.
    \item Перейти к шагу два.
\end{enumerate}

Все известные методы опираются при выборе продукции нетерминала в пункте 4 на вероятности, то есть каждой продукции каждого нетерминала задаётся вероятность её выбора. Разные алгоритмы отличаются друг от друга способом задания этой вероятности. 

\begin{itemize}
\item Выбирать продукцию равновероятно для каждого нетерминала. Плюсы - простая реализация. Минусы - часто будут генерироваться похожие тесты, медленно покрываюся всевозможные ветки деревьев. Например: если у стартового символа одна из продукций - один терминальный символ, то большая часть сгенерированных тестов будет состоять из этого символа.
\item Алгоритм построения похожих\cite{litlink1}. Если имеется какое-то множество примеров, то каждый из них представляется в виде AST и для каждого нетерминала для каждой продукции подсчитывется частота её встречаемости. По этим частотам можно вычисляется вероятность выбора каждой продукции. Чем чаще встречается переход в тестах, тем чаще он будет использоваться. Плюсы - этот метод позволяет направлять фаззер, путём изменения множества. Выбор символа становится более осмысленным с точки зрения программирования. Минусы - требуется начальное множество тестов.
\item Алгоритм построения отличных\cite{litlink1}. Отличие этого способа от предыдущего - вероятность выбора становится обратно пропорциональна частоте встречаемости, то есть чем чаще встречается переход в тестах, тем реже он будет генерироваться. Это позволяет двигаться фаззеру в противоположном направлении. Плюсы - чаще будут встречаться неожиданные пути в грамматике. Минусы - чаще будут попадаться неинтересные символы. Например, в javascript часто будет вызываться return. 
\item Ещё один способ - задать вероятности пропорционально количеству возможных поддеревьев в продукции. Если в вершине потенциально много поддеревьев, она будет чаще выбираться.
Плюсы - позволяет наиболее полно покрыть грамматику. 
Минусы - требуется предпосчёт количества деревьев для каждого нетерминала.
\end{itemize}

\subsubsection{Свойства сгенерированных тестов}
\indent

Сгенерированные тесты должны не только быть синтаксически корректными, но и обладать полезными для fuzz-тестирования свойствами.

\begin{itemize}
\item Нужно стремиться создавать короткие тесты. Чем тест длиннее - тем дольше он будет выполняться в SUT, тем сложнее его анализировать, тем дольше будет его обработка. 
\item Чем тест сложнее и рекурсивнее, тем вероятнее найти на нём ошибку SUT. 
\end{itemize}

Эти свойства не противоречат друг другу. Рассмотрим примеры для грамматики с рисунка \ref{SimpleGrammar}. Выражение "5+5+5+5+5+5+5+5+5+5+5+5" является длинным, но не сложным. Скорее всего при парсинге будет вызвана функция обработки знака "+" несколько раз последовательно, что лишь увеличит время работы программы. Другой пример - выражение "(5 + (5) + ((5 + 5) + 5))". Оно короче предыдущего, но при обработке скобок некоторые функции будут вызваны рекурсивно, что увеличивает вероятность найти ошибку.

\subsection{Получение обратной связи от запуска теста на SUT}
\indent

Получение обратной связи от запуска теста - важная часть фаззера. Именно она часто позволяет определять значимость входного набора данных. Метрики, которые полезно оценивать:

\begin{itemize}
\item Самая весомая метрика - возникновение в SOT искомых исключений,
например double free\footnote[1]{\href{https://owasp.org/www-community/vulnerabilities/Doubly\_freeing\_memory}{https://owasp.org/www-community/vulnerabilities/Doubly\_freeing\_memory}} в языке программирования c++. Зачастую такие ошибки означают, что найден баг и этот тест нужно дополнительно исследовать вручную.
\item Покрытие кода(line coverage) и покрытие функций(function coverage) - подсчёт количества строк/функций, которые покрыл тест. В контесте множества тестов можно выявлять те, которые попадают в новые строки/функции. Разным строкам и функциям можно давать разный вес при подсчёте метрики. Это позволяет направлять фаззер в сторону исследования этих функций.
\item Покрытие веток(путей) - более сложный вариант предыдущей метрики. При подсчёте учитывается последовательность выполнения строк кода или вызывов функций. Плюсы - покрытие веток гораздо более информативная метрика, чем предыдущая. Минусы - число веток растёт очень быстро с увеличением количества кода в SUT.
\end{itemize}

\subsection{Оценка полезности теста}
\indent

После запуска теста требуется узнать полезен ли этот тест или от него можно отказаться. Для этого нужно научиться сравнивать различные входные данные друг с другом.
Для этого введём функцию $value : \mathbb {X} \rightarrow \mathbb {R}$, которая каждому тесту ставит в соответствие его численную оценку. Конкретных реализаций этой функции может быть много. Приведём основные параметры, от которых она может зависеть:
\begin{itemize}
    \item При наличии исключения при запуске, значение функции становится бесконечным.
    \item Чем больше покрытие кода, тем больше значение функции. Если тест покрывает новый участок кода, то функцию можно сделать равной бесконечности.
    \item Чем короче тест, тем больше значение функции.
    \item Чем тест разнообразнее, то есть чем больше символов грамматики он покрывает, тем лучше.
\end{itemize}

\subsection{Способы обработки тестового множества}
\indent

Существует две стратегии обработки тестового множества:

\begin{enumerate}
\item Первая - наиболее часто встречающаяся - обрабатывать каждый тест по отдельности. Это позволяет распараллелить процесс fuzz-тестирования, что серьёзно его ускоряет. 
\item Вторая стратегия основана на теории эволюции Дарвина. Выбирается множество тестов. Для них всех рассчитывается функция полезности. Затем начинается процесс "выживания". Какой-то процент (например 10) тестов с наибольшей функцией полезности объявляется выжившими. Остальные случайным образом делятся на группы, в которых выживает несколько сильнейших. Потом среди оставшихся несколько случайных тестов объявляются выжившими. Тесты, которые не выжили отбрасываются.
\end{enumerate}

\subsection{Методы улучшения тестов}
\indent

Все успешные тесты необходимо преобразовывать для продолжения процесса fuzz-тестирования. Целями улучшения могут быть: 
\begin{itemize}
\item Поиск наиболее оптимального теста с точки зрения фаззера, обладающего теми же свойствами, что и улучшаемый. Другими словами - оптимизация процесса fuzz-тестирования.
\item Дальнейшее продвижение фаззера, в том числе выход из локальных экстремумов.
\end{itemize}

\subsubsection{Уменьшение размера теста}
\indent

Как было отмечено ранее, у коротких тестов есть ряд преимуществ по сравнению с длинными. Уменьшенный тест должен сохранить все полезные свойства длинного. Например, если старый тест покрывает какую-то новую функцию, то и новый должен покрывать эту функцию. Стандартные методы минмизации:

\begin{itemize}
\item Самый простой способ заключается в построении AST теста и поочерёдном удалении поддеревьев. Если после удаления поддерева, полезность теста не уменьшилась, то старый тест заменяется на новый, и продолжается процесс минимизации. Плюсы - неплохая скокорость работы. С небольшой вероятностью тест  существенно укоротится. Минусы - удаление поддерева может сделать тест синтаксически некорректным. Этот способ охватывает только очень локальные изменения теста.
\item Уменьшение поддерева. Способ похож на предыдущий, только вместо отбрасывания поддерева, оно заменяется на более короткое. Новый тест всегда будет синтаксически корректным, но перебор всех поддеревьев может занять длительное время.
\item Рекурсивное замещение поддерева. По сути этот способ является эвристикой предыдущего. Если в вершине у нетерминала F есть сын F, то производится замена поддерева текущей вершины на поддерево сына. Такое изменение оставит тест синтаксически корректным и приведёт к его упрощению для фаззера. Минус - этот способ может быть редко применим.
\item Контролировать размер во время построение теста. Этот способ применим, если используется генерация тестов. Тогда этап уменьшения можно опустить.
\end{itemize}

\subsubsection{Мутации}
\indent

Мутации являются двигателем фаззера, позволяют ему эволюционировать. Если фаззер "застрял" на каком-то этапе, то мутации могут сделать шаг в сторону и процесс fuzz-тестирования продолжится. Если тесты представимы в виде AST, то удобно описывать мутации изменениями над AST. 

\begin{itemize}
\item Скрещивание тестов друг с другом. Берустя два успешных теста, представляются в виде AST, и поддерево одного теста заменяется на поддерево другого согласно правилам грамматики. Такое изменение, например, позволяет быстро увеличивать покрытие веток SUT.   
\item Создаётся пул поддеревьев. Для каждой вершинки в AST теста считается  какая-то дополнительная информация. Поддерево этой вершинки может быть заменено только на поддерево из пула с такой же информацией. Например, информацией может быть тип нетерминала, количество идентификаторов. В пул тестов добавляются поддеревья из тестов, которые дают существенное улучшение полезности. Эта мутация позволяет обращать больше внимания на прогресс в фаззере. Например, если тестом покрыта какая-то новая функция, то среди новых тестов будет много тех, которые эту функцию исследуют. Сохранение дополнительной информации позволяет увеличить вероятность попадания в новую функцию.
\item Рекурсивное дублирование поддерева. Это обратное действие одному из способов уменьшения. Эта мутация направлена на усложнение структуры выполняемого кода в SUT, например, на создание вложенных циклов или рекурсии. 
\item Применимы стандартные AFL\footnote[1]{\href{https://github.com/google/AFL}{https://github.com/google/AFL}} мутации - бит флип, замена "интересных значений". Они более случайны, их имеет смысл применять, когда остальные не работают. После применения этой мутации тест может стать синтаксически некорректным.
\item Замена поддерева на случайно сгенерированное. Эта мутация - адаптированная для fuzz-тестирования с грамматикой версия предыдущего пункта. Основное отличие - тест останется корректным.
\item При использовании метода генерации тестов по вероятностной грамматике можно изменять вероятности генерации продукции для конкретного нетерминала. Можно изменять на случайные, можно просто немного прибавить или отнять вероятности у нескольких символов. Новые тесты могут сильно отличаться от предыдущих. Плюс такой мутации - не нужно работать с самими тестами, достаточно просто изменить вероятность при генерации.  
\end{itemize}


\newpage
\section{Анализ инструментов предметной области}
\indent

Цель анализа - рассмотреть работу инструментов, которые применяются при проверке ограничений в схемах арифметизации. В обзоре рассмотрены самые цитируемые актуальные статьи с google scholar.

\subsection{$QED^2$}
\indent

$QED^2$\cite{litlink7} - новый метод поиска ошибок в ZKP схемах, вызванных недостаточно ограниченными полиномиальными уравнениями над конечными полями. Метод выполняет семантический анализ уравнений конечного поля, сгенерированных компилятором, чтобы доказать, является ли каждый сигнал уникальным для входных данных. Подход сочетает в себе SMT-решение с упрощенным выводом уникальности, что позволяет эффективно
анализировать схемы. В работе рассмотрена конкретная реализация этого метода - инструмент Picus\footnote[1]{\href{https://github.com/chyanju/Picus}{https://github.com/chyanju/Picus}}.

\begin{figure}[ht!]
\includegraphics[width=180mm]{QED_algo.png}
\caption{Алгоритм работы метода $QED^2$}
\label{QED_algo}
\end{figure}

Инструмент принимает на вход $R1CS$ схему и пытается определить достаточно ли в ней ограничений. Результат работы инструмента может быть одним из следующих:

\begin{itemize}
    \item Ограничений достаточно. Это означает, что все скрытые переменные однозначно определяются входными, и инструмент смог это математически подтвердить.
    \item Ограничений недостаточно. В этом случае инструмент предоставит для конкретных входных значений два набора скрытых переменных, которые удовлетворяют схеме.
    \item Неопределённость. Инструмент не может дать точный ответ, так как SMT-solver не смог найти решение задачи за выделенное время.
\end{itemize}

Рассмотрим подробнее алгоритм работы инструмента, представленный на рисунке \ref{QED_algo}.

На вход алгоритму подаётся схема $C(I,W,O)$, где $I$ - входные переменные, $W$ - промежуточные, $O$ - выходные. $K$ - множество переменных, которые однозначно задаются входными данными, независимо от их значения. Изначально в $K$ лежат все входные переменные. По сути инструмент использует своеобразный метод абстрактной интерпретации - каждая переменная либо однозначно определена, тогда она лежит во множестве $K$, либо не определена. При этом во время работы метода поддерживается второе состояние абстрактной интерпретации - список возможных значений переменной. Изначально всем переменным присвоены все значения их поля $F_p$, то есть интервал $0..p-1$. Далее в цикле выполняются следующие два шага, цель каждого из которых - расширить множество $K$:

\begin{figure}[ht!]
    \includegraphics[width=180mm]{QED_rules.png}
    \caption{Правила вывода для абстрактной интерпретации определённости переменной}
    \label{QED_rules}
\end{figure}

\begin{figure}[ht!]
    \includegraphics[width=180mm]{QED_values.png}
    \caption{Правила вывода для абстрактной интерпретации значений переменной}
    \label{QED_values}
\end{figure}

\begin{enumerate}
    \item Шаг распространения определённости на основе правил. На этом шаге используются особенности схемы арифметизации $R1CS$, а именно - 4 правила вывода, которые эффективно работают на схемах. Правила представлены на рисунке \ref{QED_rules}. Этот шаг работает до тех пор, пока какое-то из правил работает и в множество $K$ добавляются новые переменные. Если правила неприменимы, то инструмент переходит к следующему шагу. При это параллельно вычисляется абстрактная интерпретация значений переменных по правилам на рисунке \ref{QED_values}.
    \item Далее каждую из неопределённых переменных алгоритм пытается доопределить при помощи солвера. Для этого формируется вторая схема $C'$ - полная копия первой, кроме названий переменных. Чтобы добавить в формулу для солвера знания о переменных, накопленные на предыдущих шагах алгоритма, определяются две формулы:
    \begin{enumerate}
        \item $\phi = \land_{u \in K} (u = u')$ - формула сужает перебор - уже определённые переменные должны быть равны.
        \item - кодируем в формулу все имеющиеся абстрактные интерпретации значений для каждой из переменных.
    \end{enumerate}
    Итоговую формула отдаётся решателю CVC5\footnote[1]{\href{https://github.com/cvc5/cvc5}{https://github.com/cvc5/cvc5}}
\end{enumerate}


Среди схем, которые может решить $QED^2$, $QED^2$ возвращает результаты, подтвержденные для подавляющего большинства - $89\%$.
Этот результат ожидаем, поскольку многие схемы, входящие в состав circomlib-utils и circomlib-core, написаны криптографами, которые также являются экспертами Circom. Однако существует 13 схем, для
которых $QED^2$ выдает контрпримеры, что означает, что эти схемы доказуемо недостаточно ограничены.

\subsection{Выводы}
\indent

Для fuzz-тестирования шаблонизаторов предлагается использовать наиболее отличившиеся идеи. Среди будущих мутаций обязательно должны присутствовать скрещивание и замена поддерева на случайное, потому что они выделяются среди остальных. Для генерации тестов можно использовать алгоритм построения похожих, вероятности вычислить по реальным программам. Для борьбы с семантическими ошибками можно использовать идеи из Grammarinator. Среди стратегий обработки тестов нет явно выделяющейся, но идеи, описанные в EvoGFuzz, мало изучены, предлагается исследовать именно их.

\newpage
\section{Результаты}
\indent

В рамках данной работы были получены следующие результаты:
\begin{itemize}
    \item Сделан обзор методов fuzz-тестирования программ, принимающих на вход данные, порождаемые КС-грамматикой. Выделены критерии для сравнения таких фаззеров.
    \item Произведён анализ существующих инструментов. На его основе предложен алгоритм для fuzz-тестирования шаблонизаторов.
\end{itemize}

\newpage

\begin{thebibliography}{}
    \addcontentsline{toc}{section}{Список литературы}
    \bibitem{litlink1}  PlonK: Permutations over Lagrange-bases for Oecumenical Noninteractive arguments of Knowledge [Электронный ресурс]. URL: \href{https://eprint.iacr.org/2019/953.pdf}{https://eprint.iacr.org/2019/953.pdf} (дата обращения: 26.05.2024)
    \bibitem{litlink2}  Automated Analysis of Halo2 Circuit [Электронный ресурс]. URL: \href{https://ceur-ws.org/Vol-3429/paper3.pdf}{https://ceur-ws.org/Vol-3429/paper3.pdf} (дата обращения: 26.05.2024)
    \bibitem{litlink3}  Certifying Zero-Knowledge Circuits with Refinement Types [Электронный ресурс]. URL: \href{https://eprint.iacr.org/2023/547.pdf}{https://eprint.iacr.org/2023/547.pdf} (дата обращения: 26.05.2024)
    \bibitem{litlink4}  SoK: What don’t we know? Understanding Security Vulnerabilities in SNARKs [Электронный ресурс]. URL: \href{https://arxiv.org/pdf/2402.15293}{https://arxiv.org/pdf/2402.15293} (дата обращения: 26.05.2024)
    \bibitem{litlink5}  Halo2 Book [Электронный ресурс]. 
    URL: \href{https://zcash.github.io/halo2/index.html}{https://zcash.github.io/halo2/index.html}
    (дата обращения: 26.05.2024)
    \bibitem{litlink6}  MoonMath manual [Электронный ресурс]. URL: \href{https://leastauthority.com/community-matters/moonmath-manual/}{https://leastauthority.com/community-matters/moonmath-manual/} (дата обращения: 26.05.2024)
    \bibitem{litlink7}  Automated Detection of Under-Constrained Circuits in Zero-Knowledge Proofs [Электронный ресурс]. URL: \href{https://eprint.iacr.org/2023/512.pdf}{https://eprint.iacr.org/2023/512.pdf} (дата обращения: 26.05.2024)
    \bibitem{litlink8}  Compositional Formal Verification
    of Zero-Knowledge Circuits [Электронный ресурс]. URL: \href{https://eprint.iacr.org/2023/1278.pdf}{https://eprint.iacr.org/2023/1278.pdf} (дата обращения: 26.05.2024)
    \bibitem{litlink9}  Formal Verification of Zero-Knowledge Circuits [Электронный ресурс]. URL: \href{https://arxiv.org/pdf/2311.08858}{https://arxiv.org/pdf/2311.08858}(дата обращения: 26.05.2024)
    \bibitem{litlink10}  Franklyn Wang (2022): Ecne: Automated Verification of ZK Circuits. OxPARC Blog [Электронный ресурс]. URL: \href{https://0xparc.org/blog/ecne}{https://0xparc.org/blog/ecne}(дата обращения: 26.05.2024)
    \bibitem{litlink11}  Plookup: A simplified polynomial protocol for
    lookup tables [Электронный ресурс]. URL: \href{https://eprint.iacr.org/2020/315.pdf}{https://eprint.iacr.org/2020/315.pdf}(дата обращения: 26.05.2024)
    \bibitem{litlink12}  ZERO-KNOWLEDGE ROLLUPS [Электронный ресурс]. URL: \href{https://ethereum.org/en/developers/docs/scaling/zk-rollups/}{https://ethereum.org/en/developers/docs/scaling/zk-rollups/}(дата обращения: 26.05.2024)
\end{thebibliography}

\end{document}